#-------------------------------------------------------------------------------
# Name:        module1
# Purpose:
#
# Author:      extra
#
# Created:     15/02/2022
# Copyright:   (c) extra 2022
# Licence:     <your licence>
#-------------------------------------------------------------------------------

import numpy as np
np.random.seed(42)

#Gradient Descent Steps
def MSEStep(X, y, W, b, learning_rate = 0.005):
	"""
	This function implements the gradient descent step
	for squared error as a performance metric.

	Parameters
	X : array of predictior features
	y : array of outcome values
	W : predictor feature coeffients
	b : regression function intercept

	Returns:
	W_new : predictor feature coefficients following gradient descent step
	b_new : intercept following gradient descent step
	"""
	# Make the predictions
	y_pred = X.dot(W) + b
	# Compute the Error
	error  = y - y_pred
	# Get the partial derivatives
	dW     = - error.dot(X)
	db     = - error.sum()
	# Update the weights using the given `learning_rate`
	W_new  = W - learning_rate * dW
	b_new  = b - learning_rate * db

	return W_new, b_new

-------------------------------------------------------------------------------
#Mini-batch implementation
def miniBatchGD(X, y,
				batch_size = 20,
				learn_rate = 0.005,
				num_iter = 25):
    """
    This function performs mini-batch gradient descent on a given dataset.

    Parameters
    X : array of predictor features
    y : array of outcome values
    batch_size : how many data points will be sampled for each iteration
    learn_rate : learning rate
    num_iter : number of batches used

    Returns
    regression_coef : array of slopes and intercepts generated by gradient
      descent procedure
    """
    n_points = X.shape[0]
    W = np.zeros(X.shape[1]) # coefficients
    b = 0 # intercept

    # run iterations
    regression_coef = [np.hstack((W,b))]
    for _ in range(num_iter):
        batch = np.random.choice(range(n_points), batch_size)
        X_batch = X[batch,:]
        y_batch = y[batch]
		# Use gradient descent
        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)
        regression_coef.append(np.hstack((W,b)))

    return regression_coef